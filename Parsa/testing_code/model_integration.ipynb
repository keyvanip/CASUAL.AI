{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67c17dd6-5125-435e-b146-40cbc5236f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/ec2-user/SageMaker/spring-2025-final-project-project-group-4\n"
     ]
    }
   ],
   "source": [
    "####################### IMPORTING ALL LIBRARIES #############################\n",
    "# Core\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "# Math & Analysis\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Image Processing\n",
    "from PIL import Image\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Vision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "# Similarity Search\n",
    "import faiss\n",
    "\n",
    "##########################################################################\n",
    "# Setting working directory\n",
    "os.chdir(\"/home/ec2-user/SageMaker/spring-2025-final-project-project-group-4\")\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76289717-7c58-401b-9a35-a62c6ac56918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outfit 1 — Score: 0.587\n",
      " Top: 24_shirts.jpg (shirt, red, casual)\n",
      " Bottom: 02_shorts.jpg (shorts, white, ['sporty'])\n",
      "\n",
      "Outfit 2 — Score: 0.537\n",
      " Top: 24_shirts.jpg (shirt, red, casual)\n",
      " Bottom: 11_pants.jpg (pants, white, ['casual'])\n",
      "\n",
      "Outfit 3 — Score: 0.529\n",
      " Top: 02_shorts.jpg (shorts, white, ['sporty'])\n",
      " Bottom: 05_t-shirts.jpg (t-shirt, black, sporty)\n",
      "\n",
      "Outfit 4 — Score: 0.527\n",
      " Top: 27_shirts.jpg (shirt, gray, casual)\n",
      " Bottom: 02_shorts.jpg (shorts, white, ['sporty'])\n",
      "\n",
      "Outfit 5 — Score: 0.517\n",
      " Top: 02_shorts.jpg (shorts, white, ['sporty'])\n",
      " Bottom: 02_t-shirts.jpg (t-shirt, white, sporty)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Load Models\n",
    "# ------------------------------\n",
    "\n",
    "# === Feature model architecture (with final FC replaced by Identity for embedding extraction) ===\n",
    "class FeatureModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.base.fc = nn.Linear(self.base.fc.in_features, 4)  # 4 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base(x)\n",
    "\n",
    "# === Instantiate and load feature model ===\n",
    "feature_model = FeatureModel()\n",
    "feature_model.load_state_dict(torch.load(\"Parsa/checkpoint/feature_model.pth\", map_location=torch.device(\"cpu\")))\n",
    "feature_model.eval()\n",
    "\n",
    "# Extract features (remove final classification layer)\n",
    "feature_extractor = nn.Sequential(*list(feature_model.base.children())[:-1])  # Output: [batch, 512, 1, 1]\n",
    "\n",
    "# === Siamese model architecture ===\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.fc(input1)\n",
    "        output2 = self.fc(input2)\n",
    "        return output1, output2\n",
    "\n",
    "# === Instantiate and load Siamese model ===\n",
    "siamese_model = SiameseNetwork()\n",
    "siamese_model.load_state_dict(torch.load(\"Parsa/checkpoint/siamese_model.pth\", map_location=torch.device(\"cpu\")))\n",
    "siamese_model.eval()\n",
    "\n",
    "# ------------------------------\n",
    "# Image Preprocessing\n",
    "# ------------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def load_image_embedding(img_path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_tensor = transform(img).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        features = feature_extractor(img_tensor).squeeze().numpy()\n",
    "    return features\n",
    "\n",
    "# ------------------------------\n",
    "# Load Metadata\n",
    "# ------------------------------\n",
    "with open(\"Parsa/parsa's_wardrobe/metadata.json\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "wardrobe_items = metadata[\"shirts\"] + metadata.get(\"pants\", []) + metadata.get(\"shorts\", []) + metadata.get(\"t-shirts\", [])\n",
    "\n",
    "# ------------------------------\n",
    "# Generate Embeddings\n",
    "# ------------------------------\n",
    "embeddings = []\n",
    "paths = []\n",
    "folder_map = {\n",
    "    \"shirt\": \"shirts\",\n",
    "    \"t-shirt\": \"t-shirts\",\n",
    "    \"short\": \"shorts\",\n",
    "    \"pant\": \"pants\",  # In case any metadata used singular form\n",
    "    \"pants\": \"pants\",\n",
    "    \"shorts\": \"shorts\",\n",
    "    \"shirts\": \"shirts\",\n",
    "    \"t-shirts\": \"t-shirts\"\n",
    "}\n",
    "\n",
    "for item in wardrobe_items:\n",
    "    subfolder = folder_map.get(item[\"category\"], item[\"category\"])\n",
    "    filepath = os.path.join(\"Parsa/parsa's_wardrobe\", subfolder, item[\"filename\"])\n",
    "    emb = load_image_embedding(filepath)\n",
    "    embeddings.append(emb)\n",
    "    paths.append(item)\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "\n",
    "# Save embeddings (optional)\n",
    "with open(\"Parsa/testing_code/clothing_embeddings.json\", \"w\") as f:\n",
    "    json.dump([{**paths[i], \"embedding\": embeddings[i].tolist()} for i in range(len(paths))], f)\n",
    "\n",
    "# ------------------------------\n",
    "# Build FAISS Index\n",
    "# ------------------------------\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "faiss.write_index(index, \"Parsa/testing_code/clothing_faiss.index\")\n",
    "\n",
    "# ------------------------------\n",
    "# Outfit Matching Logic\n",
    "# ------------------------------\n",
    "def is_compatible_pair(item1, item2):\n",
    "    if item1[\"category\"] in [\"shirt\", \"t-shirt\"] and item2[\"category\"] in [\"pants\", \"shorts\"]:\n",
    "        return True\n",
    "    if item2[\"category\"] in [\"shirt\", \"t-shirt\"] and item1[\"category\"] in [\"pants\", \"shorts\"]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_top_outfits(top_k=5):\n",
    "    outfit_scores = []\n",
    "    for i in range(len(paths)):\n",
    "        for j in range(i+1, len(paths)):\n",
    "            if is_compatible_pair(paths[i], paths[j]):\n",
    "                emb1 = torch.tensor(embeddings[i]).unsqueeze(0)\n",
    "                emb2 = torch.tensor(embeddings[j]).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    output1, output2 = siamese_model(emb1, emb2)\n",
    "                    score = torch.nn.functional.pairwise_distance(output1, output2).item()\n",
    "                outfit_scores.append(((paths[i], paths[j]), score))\n",
    "\n",
    "    outfit_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return outfit_scores[:top_k]\n",
    "\n",
    "# ------------------------------\n",
    "# Show Recommendations\n",
    "# ------------------------------\n",
    "top_outfits = get_top_outfits(top_k=5)\n",
    "\n",
    "for idx, ((item1, item2), score) in enumerate(top_outfits):\n",
    "    print(f\"\\nOutfit {idx+1} — Score: {score:.3f}\")\n",
    "    print(f\" Top: {item1['filename']} ({item1['category']}, {item1['color']}, {item1['style']})\")\n",
    "    print(f\" Bottom: {item2['filename']} ({item2['category']}, {item2['color']}, {item2['style']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dfac84-ca8e-4fc4-b92e-4a1fd4e9e9c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
