{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d47ddcdc-45ff-4fc5-a179-b4350722c90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/ec2-user/SageMaker/spring-2025-final-project-project-group-4\n"
     ]
    }
   ],
   "source": [
    "####################### IMPORTING ALL LIBRARIES #############################\n",
    "# Core\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "# Math & Analysis\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Image Processing\n",
    "from PIL import Image\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Vision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "# Similarity Search\n",
    "import faiss\n",
    "\n",
    "##########################################################################\n",
    "# Setting working directory\n",
    "os.chdir(\"/home/ec2-user/SageMaker/spring-2025-final-project-project-group-4\")\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca028d9-9b76-408e-b6f4-09490a30f81f",
   "metadata": {},
   "source": [
    "## 1. File Renaming for Clean Image Format\n",
    "### Standardize all image filenames by category to include a suffix like _shorts, _pants, etc., so that they match the format expected in metadata and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d94e43f4-dd27-47e9-9097-9add9913b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Rename shorts\n",
    "cd Parsa/clothes/shorts\n",
    "for i in {1..40}; do mv \"$(printf \"%02d\" $i).jpg\" \"$(printf \"%02d\" $i)_shorts.jpg\"; done\n",
    "\n",
    "# Rename pants\n",
    "cd ../pants\n",
    "for i in {1..40}; do mv \"$(printf \"%02d\" $i).jpg\" \"$(printf \"%02d\" $i)_pants.jpg\"; done\n",
    "\n",
    "# Rename shirts\n",
    "cd ../shirts\n",
    "for i in {1..40}; do mv \"$(printf \"%02d\" $i).jpg\" \"$(printf \"%02d\" $i)_shirts.jpg\"; done\n",
    "\n",
    "# Rename t-shirts\n",
    "cd ../t-shirts\n",
    "for i in {1..40}; do mv \"$(printf \"%02d\" $i).jpg\" \"$(printf \"%02d\" $i)_t-shirts.jpg\"; done\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a39834e-e756-4917-946f-bca90885a000",
   "metadata": {},
   "source": [
    "## 2. Image Integrity Check\n",
    "### Verify that every image referenced in our metadata file exists in the corresponding directory. This helps you catch any missing or misnamed images before training begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdbe10bc-8454-4688-bb1e-10279a043fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Missing file: Parsa/clothes/t-shirts/05_t-shirts.jpg \n",
      "\n",
      "‚úÖ Total images referenced in metadata: 160\n",
      "‚ùå Missing images: 1\n"
     ]
    }
   ],
   "source": [
    "# === Check if all images in metadata exist ===\n",
    "metadata_path = \"metadata.json\"\n",
    "data_dir = \"Parsa/clothes\"\n",
    "\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "total_images = 0\n",
    "missing_images = 0\n",
    "\n",
    "for category, items in metadata.items():\n",
    "    category_dir = os.path.join(data_dir, category)\n",
    "    if not os.path.isdir(category_dir):\n",
    "        print(f\"üö® Folder missing: {category_dir}\")\n",
    "        continue\n",
    "\n",
    "    for item in items:\n",
    "        filename = item.get(\"filename\")\n",
    "        if not filename:\n",
    "            continue\n",
    "\n",
    "        full_path = os.path.join(category_dir, filename)\n",
    "        total_images += 1\n",
    "\n",
    "        if not os.path.isfile(full_path):\n",
    "            print(f\"‚ùå Missing file: {full_path}\")\n",
    "            missing_images += 1\n",
    "\n",
    "print(f\"\\n‚úÖ Total images referenced in metadata: {total_images}\")\n",
    "print(f\"‚ùå Missing images: {missing_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83fcefd-58fa-4d25-befb-efab7ce08ae7",
   "metadata": {},
   "source": [
    "## 3. Configuration & Transforms\n",
    "### Set up project-level constants including batch size, epochs, device (CPU/GPU), and normalization transforms. These will be used consistently throughout the dataset, model, and training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4dd0539-2b84-4bbd-a7b3-376094e18f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configurations ===\n",
    "metadata_path = \"metadata.json\"\n",
    "data_dir = \"Parsa/clothes\"\n",
    "num_classes = 4  # pants, shorts, shirt, t-shirt\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "learning_rate = 0.0005\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Image Transforms ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a5e55a-de9a-4781-91e4-4fc0c886ab64",
   "metadata": {},
   "source": [
    "## 4. Dataset Loader\n",
    "### Create a custom PyTorch Dataset that reads metadata, loads images from disk, applies transforms, and labels them based on category. This feeds into the DataLoader for efficient batching and shuffling during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c57dea85-f1fb-409a-b297-b86ffe2aa0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Custom Clothing Dataset ===\n",
    "class ClothingDataset(Dataset):\n",
    "    def __init__(self, metadata, root_dir, transform=None):\n",
    "        self.samples = []\n",
    "        self.label_map = {\"pants\": 0, \"shorts\": 1, \"shirts\": 2, \"t-shirts\": 3}\n",
    "        self.transform = transform\n",
    "\n",
    "        for category, items in metadata.items():\n",
    "            if category not in self.label_map:\n",
    "                continue\n",
    "            for item in items:\n",
    "                image_path = os.path.join(root_dir, category, item[\"filename\"])\n",
    "                if os.path.exists(image_path):\n",
    "                    self.samples.append((image_path, self.label_map[category]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# === Load Metadata + DataLoader ===\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "dataset = ClothingDataset(metadata, data_dir, transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2271bb31-22f8-4a17-9c43-6368dd6b6bff",
   "metadata": {},
   "source": [
    "## 5. Define and Train Model\n",
    "\n",
    "### Build a lightweight ResNet18-based classification model, replace its final layer for 4-class clothing prediction, and train it using cross-entropy loss. Track loss and accuracy over multiple epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3b74347-bb7e-42ae-a21b-d1257f05674e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.6899, Accuracy: 78.62%\n",
      "Epoch [2/10], Loss: 0.2335, Accuracy: 98.74%\n",
      "Epoch [3/10], Loss: 0.0381, Accuracy: 100.00%\n",
      "Epoch [4/10], Loss: 0.1027, Accuracy: 99.37%\n",
      "Epoch [5/10], Loss: 0.1076, Accuracy: 98.74%\n",
      "Epoch [6/10], Loss: 0.0636, Accuracy: 98.74%\n",
      "Epoch [7/10], Loss: 0.0301, Accuracy: 100.00%\n",
      "Epoch [8/10], Loss: 0.0284, Accuracy: 100.00%\n",
      "Epoch [9/10], Loss: 0.0243, Accuracy: 100.00%\n",
      "Epoch [10/10], Loss: 0.0804, Accuracy: 99.37%\n",
      "\n",
      "‚úÖ Model saved at Parsa/checkpoint/feature_model.pth\n"
     ]
    }
   ],
   "source": [
    "# === Define CNN Feature Model ===\n",
    "class FeatureModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.base.fc = nn.Linear(self.base.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base(x)\n",
    "\n",
    "model = FeatureModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# === Train Model ===\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss:.4f}, Accuracy: {acc:.2f}%\")\n",
    "\n",
    "# === Save Trained Model ===\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"Parsa/checkpoint/feature_model.pth\")\n",
    "print(\"\\n‚úÖ Model saved at Parsa/checkpoint/feature_model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8361637a-c345-4744-bb03-494faf50901a",
   "metadata": {},
   "source": [
    "## 6. Generate Embeddings\n",
    "### Use the trained model (minus its classification head) as a feature extractor to generate 512D vector embeddings for each image. These embeddings are saved as a dictionary to use later in similarity matching or pairing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1620272-663e-454c-b6c3-bb0ca6324d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings saved to Parsa/checkpoint/clothing_embeddings.json\n"
     ]
    }
   ],
   "source": [
    "# === Extract Feature Embeddings ===\n",
    "model.eval()\n",
    "feature_extractor = nn.Sequential(*list(model.base.children())[:-1]) \n",
    "\n",
    "# === Prepare storage ===\n",
    "embedding_dict = {} \n",
    "embedding_list = [] \n",
    "image_list = []      \n",
    "\n",
    "# === Extract embeddings from dataset ===\n",
    "with torch.no_grad():\n",
    "    for img_path, label in dataset.samples:\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = transform(image).unsqueeze(0).to(device)  # [1, 3, 224, 224]\n",
    "\n",
    "        features = feature_extractor(image)  # Output shape: [1, 512, 1, 1]\n",
    "        embedding = features.squeeze().cpu().numpy()  # Convert to [512]\n",
    "\n",
    "        embedding_dict[img_path] = embedding.tolist()  # Save for JSON\n",
    "        embedding_list.append(embedding)\n",
    "        image_list.append(img_path)\n",
    "\n",
    "# === Save embeddings to JSON ===\n",
    "with open(\"Parsa/checkpoint/clothing_embeddings.json\", \"w\") as f:\n",
    "    json.dump(embedding_dict, f)\n",
    "print(\"‚úÖ Embeddings saved to Parsa/checkpoint/clothing_embeddings.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968921ba-56c6-4e29-bf14-0399d172421b",
   "metadata": {},
   "source": [
    "## 7. Build and Save FAISS Index\n",
    "\n",
    "### Convert all embeddings into a numpy array and store them in a FAISS index, enabling fast nearest-neighbor queries. Save both the index and the raw embeddings to disk for future retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "640269cd-4177-4011-8daa-58dfc5255a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS index saved to Parsa/checkpoint/clothing_faiss.index\n"
     ]
    }
   ],
   "source": [
    "# === Save to FAISS index ===\n",
    "embedding_array = np.array(embedding_list).astype(\"float32\")\n",
    "faiss_index = faiss.IndexFlatL2(embedding_array.shape[1])  # 512D L2 similarity index\n",
    "faiss_index.add(embedding_array)\n",
    "faiss.write_index(faiss_index, \"Parsa/checkpoint/clothing_faiss.index\")\n",
    "print(\"‚úÖ FAISS index saved to Parsa/checkpoint/clothing_faiss.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8850c41-6c00-4e7e-9fad-e64558975be3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
