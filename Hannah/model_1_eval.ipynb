{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a393fdd-59f7-4947-b4e5-cb34fcf54fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.10.0.tar.gz (69 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy<3.0,>=1.25.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from faiss-cpu) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from packaging->faiss-cpu) (3.2.1)\n",
      "Building wheels for collected packages: faiss-cpu\n",
      "  Building wheel for faiss-cpu (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for faiss-cpu \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[50 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /tmp/pip-build-env-kuq2sktd/overlay/lib/python3.10/site-packages/setuptools/config/_apply_pyprojecttoml.py:82: SetuptoolsDeprecationWarning: `project.license` as a TOML table is deprecated\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Please use a simple string containing a SPDX expression for `project.license`. You can also use `project.license-files`. (Both options available on setuptools>=77.0.0).\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         By 2026-Feb-18, you need to update your project and remove deprecated calls\n",
      "  \u001b[31m   \u001b[0m         or your builds will no longer be supported.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   corresp(dist, value, root_dir)\n",
      "  \u001b[31m   \u001b[0m /tmp/pip-build-env-kuq2sktd/overlay/lib/python3.10/site-packages/setuptools/config/_apply_pyprojecttoml.py:61: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         License :: OSI Approved :: MIT License\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   dist._finalize_license_expression()\n",
      "  \u001b[31m   \u001b[0m /tmp/pip-build-env-kuq2sktd/overlay/lib/python3.10/site-packages/setuptools/dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         License :: OSI Approved :: MIT License\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   self._finalize_license_expression()\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'faiss._swigfaiss' extension\n",
      "  \u001b[31m   \u001b[0m swigging faiss/faiss/python/swigfaiss.i to faiss/faiss/python/swigfaiss_wrap.cpp\n",
      "  \u001b[31m   \u001b[0m swig -python -c++ -Doverride= -doxygen -Ifaiss -I/tmp/pip-build-env-kuq2sktd/overlay/lib/python3.10/site-packages/numpy/_core/include -Ifaiss -I/usr/local/include -DSWIGWORDSIZE64 -o faiss/faiss/python/swigfaiss_wrap.cpp faiss/faiss/python/swigfaiss.i\n",
      "  \u001b[31m   \u001b[0m swig error : Unrecognized option -doxygen\n",
      "  \u001b[31m   \u001b[0m Use 'swig -help' for available options.\n",
      "  \u001b[31m   \u001b[0m error: command '/usr/bin/swig' failed with exit code 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for faiss-cpu\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hFailed to build faiss-cpu\n",
      "\u001b[31mERROR: Failed to build installable wheels for some pyproject.toml based projects (faiss-cpu)\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5407579d-56cf-4306-94b5-1b5f480980f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faiss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load Embedding Metadata\n",
    "with open('clothing_embedding.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Load FAISS Index\n",
    "index = faiss.read_index('clothing_faiss.index')\n",
    "\n",
    "# Image Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load Model for Embedding\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = torch.nn.Identity()\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def get_image_embedding(img_path):\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = model(image).cpu().numpy().flatten()\n",
    "    return emb.astype('float32')\n",
    "\n",
    "\n",
    "def search_similar_images(query_path, k=5):\n",
    "    query_emb = get_image_embedding(query_path)\n",
    "    D, I = index.search(query_emb.reshape(1, -1), k)\n",
    "    \n",
    "    print(f\"Query Image: {query_path}\")\n",
    "    show_images([query_path] + [metadata[i]['image_path'] for i in I[0]])\n",
    "\n",
    "\n",
    "def show_images(img_paths):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for idx, path in enumerate(img_paths):\n",
    "        img = Image.open(path)\n",
    "        plt.subplot(1, len(img_paths), idx+1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "search_similar_images('clothes/train/pants/01_pants.jpg', k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e0c49-931a-4ea4-9c74-aa7fa266b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load Embedding Metadata\n",
    "with open('clothing_embedding.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Load FAISS Index\n",
    "index = faiss.read_index('clothing_faiss.index')\n",
    "\n",
    "# Image Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load Model for Embedding\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = torch.nn.Identity()\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def get_image_embedding(img_path):\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = model(image).cpu().numpy().flatten()\n",
    "    return emb.astype('float32')\n",
    "\n",
    "\n",
    "def compute_top_k_accuracy(k=5):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for entry in tqdm(metadata, desc=\"Evaluating\"):\n",
    "        true_category = entry[\"category\"]\n",
    "        img_path = entry[\"image_path\"]\n",
    "        \n",
    "        # Get query image embedding\n",
    "        query_emb = get_image_embedding(img_path)\n",
    "\n",
    "        # Search in FAISS index\n",
    "        D, I = index.search(query_emb.reshape(1, -1), k)\n",
    "\n",
    "        # Check if any of the top-K results match the true category\n",
    "        retrieved_categories = [metadata[i]['category'] for i in I[0]]\n",
    "        if true_category in retrieved_categories:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    accuracy = (correct / total) * 100\n",
    "    print(f\"Top-{k} Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "\n",
    "# Compute Top-5 Accuracy\n",
    "compute_top_k_accuracy(k=5)\n",
    "\n",
    "# Top-1 Accuracy\n",
    "compute_top_k_accuracy(k=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ace9c8f-05df-49b3-9fe2-555129228fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(k=5):\n",
    "    average_precisions = []\n",
    "    for entry in tqdm(metadata, desc=\"Evaluating mAP\"):\n",
    "        true_category = entry[\"category\"]\n",
    "        img_path = entry[\"image_path\"]\n",
    "        \n",
    "        query_emb = get_image_embedding(img_path)\n",
    "        D, I = index.search(query_emb.reshape(1, -1), k)\n",
    "        \n",
    "        retrieved_categories = [metadata[i]['category'] for i in I[0]]\n",
    "        \n",
    "        # Calculate precision at each rank position\n",
    "        relevant_retrievals = [1 if retrieved_categories[i] == true_category else 0 for i in range(k)]\n",
    "        precision_at_k = np.cumsum(relevant_retrievals) / (np.arange(k) + 1)\n",
    "        \n",
    "        # Average precision for this query\n",
    "        average_precision = np.sum(precision_at_k * relevant_retrievals) / np.sum(relevant_retrievals) if np.sum(relevant_retrievals) > 0 else 0\n",
    "        average_precisions.append(average_precision)\n",
    "    \n",
    "    mAP = np.mean(average_precisions)\n",
    "    print(f\"Mean Average Precision (mAP) at {k}: {mAP:.4f}\")\n",
    "\n",
    "mean_average_precision(k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36da9613-242a-47bc-b670-7d24ff2a7e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(k=5):\n",
    "    correct_retrievals = 0\n",
    "    total_relevant = 0\n",
    "    \n",
    "    for entry in tqdm(metadata, desc=\"Evaluating Recall\"):\n",
    "        true_category = entry[\"category\"]\n",
    "        img_path = entry[\"image_path\"]\n",
    "        \n",
    "        query_emb = get_image_embedding(img_path)\n",
    "        D, I = index.search(query_emb.reshape(1, -1), k)\n",
    "        \n",
    "        retrieved_categories = [metadata[i]['category'] for i in I[0]]\n",
    "        \n",
    "        # Count how many of the relevant items (same category) are in the top K\n",
    "        relevant_retrievals = [1 if retrieved_categories[i] == true_category else 0 for i in range(k)]\n",
    "        correct_retrievals += np.sum(relevant_retrievals)\n",
    "        total_relevant += 1  # Every image has at least one relevant category (itself)\n",
    "    \n",
    "    recall = correct_retrievals / total_relevant\n",
    "    print(f\"Recall at {k}: {recall:.4f}\")\n",
    "\n",
    "recall_at_k(k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687c7332-d409-491b-ab8e-ec8ad851f73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(k=5):\n",
    "    total_precision = 0\n",
    "    \n",
    "    for entry in tqdm(metadata, desc=\"Evaluating Precision\"):\n",
    "        true_category = entry[\"category\"]\n",
    "        img_path = entry[\"image_path\"]\n",
    "        \n",
    "        query_emb = get_image_embedding(img_path)\n",
    "        D, I = index.search(query_emb.reshape(1, -1), k)\n",
    "        \n",
    "        retrieved_categories = [metadata[i]['category'] for i in I[0]]\n",
    "        \n",
    "        # Count how many of the retrieved items are relevant (same category)\n",
    "        relevant_retrievals = [1 if retrieved_categories[i] == true_category else 0 for i in range(k)]\n",
    "        \n",
    "        # Precision for this query\n",
    "        precision_at_k = np.sum(relevant_retrievals) / k\n",
    "        total_precision += precision_at_k\n",
    "    \n",
    "    avg_precision = total_precision / len(metadata)\n",
    "    print(f\"Average Precision at {k}: {avg_precision:.4f}\")\n",
    "\n",
    "precision_at_k(k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075b67bc-2fa7-49b6-9a24-0533b03af105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def evaluate_search_speed(num_queries=100, k=5):\n",
    "    total_time = 0\n",
    "    \n",
    "    for entry in tqdm(metadata[:num_queries], desc=\"Evaluating Search Speed\"):\n",
    "        img_path = entry[\"image_path\"]\n",
    "        query_emb = get_image_embedding(img_path)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        D, I = index.search(query_emb.reshape(1, -1), k)\n",
    "        total_time += time.time() - start_time\n",
    "    \n",
    "    avg_search_time = total_time / num_queries\n",
    "    print(f\"Average Search Time for {num_queries} queries: {avg_search_time:.4f} seconds\")\n",
    "\n",
    "# Example usage\n",
    "evaluate_search_speed(num_queries=100, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92fca39-2d74-416e-8eb4-c5425f6050f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def visualize_embeddings():\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    \n",
    "    # Map categories to numerical values\n",
    "    category_map = {category: idx for idx, category in enumerate(set(entry['category'] for entry in metadata))}\n",
    "    \n",
    "    for entry in tqdm(metadata, desc=\"Collecting Embeddings\"):\n",
    "        img_path = entry[\"image_path\"]\n",
    "        embedding = get_image_embedding(img_path)\n",
    "        embeddings.append(embedding)\n",
    "        labels.append(category_map[entry[\"category\"]])  # Store numerical label\n",
    "    \n",
    "    # Reduce dimensionality with PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_embeddings = pca.fit_transform(np.array(embeddings))\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Use the 'viridis' colormap, but you can choose another colormap (e.g., 'tab20')\n",
    "    scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap='tab20', alpha=0.6)\n",
    "    \n",
    "    # Create a legend using category names\n",
    "    handles, _ = scatter.legend_elements()\n",
    "    plt.legend(handles, category_map.keys(), title=\"Categories\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "visualize_embeddings()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
