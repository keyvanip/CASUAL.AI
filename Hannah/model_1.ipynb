{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "584803ab-8a4e-4f06-adb1-7145e0066b85",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d70101c6-5a0f-48c8-ba8a-f0b1ac179234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done renaming all images!\n"
     ]
    }
   ],
   "source": [
    "# Change the data name\n",
    "import os\n",
    "\n",
    "data_root = \"clothes/train\"\n",
    "categories = [\"pants\", \"shirts\", \"shorts\", \"t-shirts\"]\n",
    "\n",
    "for category in categories:\n",
    "    category_folder = os.path.join(data_root, category)\n",
    "    \n",
    "    for fname in os.listdir(category_folder):\n",
    "        if fname.endswith('.jpg') and fname.startswith(category + \"_\"):\n",
    "            parts = fname.split('_')\n",
    "            if len(parts) >= 2:\n",
    "                number = parts[1].split('.')[0]  # Extract number without .jpg\n",
    "                new_name = f\"{number}_{category}.jpg\"\n",
    "                old_path = os.path.join(category_folder, fname)\n",
    "                new_path = os.path.join(category_folder, new_name)\n",
    "                \n",
    "                print(f\"Renaming {old_path} -> {new_path}\")\n",
    "                os.rename(old_path, new_path)\n",
    "\n",
    "print(\"Done renaming all images!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04140b46-f6b1-4dab-813a-cb23f1633858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3180625b-2c56-4ff4-83f5-32316fe4d985",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c4e8a4-502c-4ffb-a6ad-15956f390176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "212c6f45-66fe-4bd7-9351-8ebfed83f176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "data_root = \"clothes/train\"\n",
    "metadata_path = \"metadata.json\"\n",
    "categories = [\"pants\", \"shirts\", \"shorts\", \"t-shirts\"]\n",
    "model_save_path = \"model_1.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2db9dfe-9440-4112-8343-25ff76b646b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "with open(metadata_path, 'r') as f:\n",
    "    metadata = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "973e8e8a-752c-4d7d-b3f2-d5fc9c346bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined dataset from all categories\n",
    "class ClothingDataset(Dataset):\n",
    "    def __init__(self, metadata, categories, data_root, transform=None):\n",
    "        self.data_root = data_root\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Map category names to numerical labels\n",
    "        self.category_to_idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "        \n",
    "        # Collect all images and their labels\n",
    "        for category in categories:\n",
    "            category_items = metadata.get(category, [])\n",
    "            for item in category_items:\n",
    "                img_path = os.path.join(data_root, category, item['filename'])\n",
    "                if os.path.exists(img_path):\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(self.category_to_idx[category])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load and transform image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2f6e9fe-d224-4969-95c0-322954325a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1c5ce00-832f-472e-8a2a-d763af322b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/ec2-user/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 124MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Create dataset and split into train/validation\n",
    "full_dataset = ClothingDataset(metadata, categories, data_root, transform)\n",
    "train_indices, val_indices = train_test_split(range(len(full_dataset)), test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Define CNN model (using ResNet-18 pretrained)\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)  # correct way for torchvision>=0.13\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, len(categories))  # 4 output classes\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2776d28-b159-49a1-bc1d-e6985d448375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.0614, Validation Accuracy: 87.50%\n",
      "Epoch 2/10, Loss: 0.0373, Validation Accuracy: 93.75%\n",
      "Epoch 3/10, Loss: 0.0298, Validation Accuracy: 93.75%\n",
      "Epoch 4/10, Loss: 0.0432, Validation Accuracy: 93.75%\n",
      "Epoch 5/10, Loss: 0.0227, Validation Accuracy: 100.00%\n",
      "Epoch 6/10, Loss: 0.0003, Validation Accuracy: 93.75%\n",
      "Epoch 7/10, Loss: 0.0117, Validation Accuracy: 87.50%\n",
      "Epoch 8/10, Loss: 0.0028, Validation Accuracy: 62.50%\n",
      "Epoch 9/10, Loss: 0.0009, Validation Accuracy: 56.25%\n",
      "Epoch 10/10, Loss: 0.0010, Validation Accuracy: 62.50%\n",
      "Model saved to model_1.pth\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d01d2ca-04aa-4b50-ad7c-4149752982fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge faiss-cpu -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7d81390-dd0c-4abb-8d9a-07bd86a1abfa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faiss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_root = \"clothes/train\"\n",
    "categories = [\"pants\", \"shirts\", \"shorts\", \"t-shirts\"]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pretrained model\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Identity()  # Remove classifier\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "embeddings = []\n",
    "metadata = []\n",
    "\n",
    "# Extract embeddings\n",
    "with torch.no_grad():\n",
    "    for category in categories:\n",
    "        folder = os.path.join(data_root, category)\n",
    "        for fname in tqdm(os.listdir(folder), desc=f\"Processing {category}\"):\n",
    "            if fname.endswith('.jpg'):\n",
    "                img_path = os.path.join(folder, fname)\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "                emb = model(image).cpu().numpy().flatten()\n",
    "                embeddings.append(emb.tolist())\n",
    "\n",
    "                metadata.append({\n",
    "                    \"image_path\": img_path,\n",
    "                    \"category\": category\n",
    "                })\n",
    "\n",
    "# Save embeddings to json\n",
    "with open('clothing_embedding.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "# Build FAISS index\n",
    "embeddings_np = np.array(embeddings).astype('float32')\n",
    "index = faiss.IndexFlatL2(embeddings_np.shape[1])\n",
    "index.add(embeddings_np)\n",
    "\n",
    "faiss.write_index(index, 'clothing_faiss.index')\n",
    "\n",
    "print(\"Done! Files saved:\")\n",
    "print(\"- clothing_embedding.json\")\n",
    "print(\"- clothing_faiss.index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1ac41f-aeaf-4ab6-890d-2db5b919e918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
