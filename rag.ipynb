{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0acf7475-eee4-43e3-906e-5ac05bd1776a",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66081f24-bb3a-457d-8e57-c7d32b814470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import logging\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f031fc-f99a-497e-8ce5-b44c9d204973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c399a0c-d275-4875-812a-37b018b031e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLE_TO_ASSUME = Path(os.path.join(os.environ[\"HOME\"],\"BedrockCrossAccount.txt\")).read_text().strip()\n",
    "logger.info(f\"ROLE_TO_ASSUME={ROLE_TO_ASSUME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d15dbf-ca7f-4092-aa17-7b77be156243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import datetime\n",
    "from botocore.session import get_session\n",
    "from botocore.credentials import RefreshableCredentials\n",
    "\n",
    "# ARN of Role A to assume  \n",
    "role_to_assume = 'arn:aws:iam::605134468121:role/BedrockCrossAccount'\n",
    "\n",
    "def get_credentials():\n",
    "    sts_client = boto3.client('sts')\n",
    "    assumed_role = sts_client.assume_role(\n",
    "        RoleArn=role_to_assume,\n",
    "        RoleSessionName='cross-account-session',\n",
    "        # Don't set DurationSeconds when role chaining\n",
    "    )\n",
    "    return {\n",
    "        'access_key': assumed_role['Credentials']['AccessKeyId'],\n",
    "        'secret_key': assumed_role['Credentials']['SecretAccessKey'],\n",
    "        'token': assumed_role['Credentials']['SessionToken'],\n",
    "        'expiry_time': assumed_role['Credentials']['Expiration'].isoformat()\n",
    "    }\n",
    "\n",
    "session = get_session()\n",
    "refresh_creds = RefreshableCredentials.create_from_metadata(\n",
    "    metadata=get_credentials(),\n",
    "    refresh_using=get_credentials,\n",
    "    method='sts-assume-role'\n",
    ")\n",
    "\n",
    "# Create a new session with refreshable credentials\n",
    "session._credentials = refresh_creds\n",
    "boto3_session = boto3.Session(botocore_session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a43508-2781-4b8f-875e-592516bb4b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "region: str = \"us-west-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b69368-191d-4b3a-afd3-6aad31b482fe",
   "metadata": {},
   "source": [
    "ChatBedrock is an API interface that lets users interact with LLMs on Amazon Bedrock, similar to how OpenAI’s API works for GPT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8539d992-2b91-44f5-9729-2713e1e83332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "import boto3\n",
    "\n",
    "# ---- ⚠️ Update region for your AWS setup ⚠️ ----\n",
    "bedrock_client = boto3_session.client(\"bedrock-runtime\",\n",
    "                              region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a16719a-0639-48f1-8201-1108172bece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatBedrockConverse(\n",
    "    client=bedrock_client,\n",
    "    model_id=\"us.amazon.nova-micro-v1:0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fee5c3e-66a8-4c88-9702-94f1734d1fb3",
   "metadata": {},
   "source": [
    "# Steps to Implement RAG for an Image Dataset:\n",
    "## 1. Preprocess Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0ef03b-abfc-456a-92a5-0fb0c0abac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load CLIP model & processor\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Folder containing images\n",
    "IMAGE_FOLDER = \"data/\"\n",
    "image_files = [f for f in os.listdir(IMAGE_FOLDER) if f.endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "\n",
    "# Function to process and get embeddings for images\n",
    "def get_image_embedding(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = model.get_image_features(**inputs)\n",
    "    return embedding.cpu().numpy().flatten()\n",
    "\n",
    "# Extract embeddings for all images\n",
    "embeddings = []\n",
    "image_paths = []\n",
    "for image_file in image_files:\n",
    "    image_path = os.path.join(IMAGE_FOLDER, image_file)\n",
    "    embedding = get_image_embedding(image_path)\n",
    "    embeddings.append(embedding)\n",
    "    image_paths.append(image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d97dc7f-6703-4725-a5c7-1386d1c9a962",
   "metadata": {},
   "source": [
    "## 2. Store Image Embeddings in a Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dccb7bc-fa0d-4cc5-9d30-65a2d285f1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embeddings to FAISS index\n",
    "embeddings = np.array(embeddings).astype(\"float32\")\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 distance (Euclidean)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(index, \"faiss_index.idx\")\n",
    "\n",
    "# Save image paths for later retrieval\n",
    "with open(\"image_paths.txt\", \"w\") as f:\n",
    "    for path in image_paths:\n",
    "        f.write(path + \"\\n\")\n",
    "\n",
    "print(f\"Stored {len(image_paths)} image embeddings in FAISS index.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d730d6-32b9-405b-aa7a-9c2edd021b16",
   "metadata": {},
   "source": [
    "## 3. Query Processing (Retrieval Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c2d01e-3287-46a1-a423-b8aa8a2fecc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f3c7220-e570-4177-9423-9dffccda5f37",
   "metadata": {},
   "source": [
    "## 4. Augmenting the Query (Generation Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6886f221-2205-4ac3-8c0e-cf00868f24ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
